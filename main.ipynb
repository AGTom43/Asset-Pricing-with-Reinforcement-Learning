{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Date  Last Price    Volume  SMAVG (15)\n",
      "2331 2023-03-07 20:00:00    0.596608  0.049707    0.072420\n",
      "2330 2023-03-07 20:30:00    0.606681  0.165330    0.086878\n",
      "2329 2023-03-08 14:30:00    0.634154  0.134774    0.099523\n",
      "2328 2023-03-08 15:00:00    0.608055  0.086473    0.077523\n",
      "2327 2023-03-08 15:30:00    0.571654  0.099766    0.067301\n",
      "...                  ...         ...       ...         ...\n",
      "473  2023-09-29 20:30:00    0.881409  0.306496    0.180971\n",
      "472  2023-10-02 14:30:00    0.794870  0.171930    0.194260\n",
      "471  2023-10-02 15:00:00    0.755034  0.144825    0.184963\n",
      "470  2023-10-02 15:30:00    0.762818  0.113712    0.166506\n",
      "469  2023-10-02 16:00:00    0.764192  0.092697    0.157100\n",
      "\n",
      "[1863 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "#C:\\Users\\rohit\\OneDrive\\Documents\\Asset-Pricing-with-Reinforcement-Learning\\XOM_30_minute_6_month_data.csv\n",
    "df = pd.read_csv('XOM_30_minute_6_month_data.csv', parse_dates=['Date'])\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "df[['Last Price', 'Volume', 'SMAVG (15)']] = scaler.fit_transform(df[['Last Price', 'Volume', 'SMAVG (15)']])\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(train_df[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class DDPGTradingEnv(gymnasium.Env):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.df = df\n",
    "    self.current_step = 0\n",
    "    self.total_steps = len(df) - 1\n",
    "\n",
    "    # Negatives mean quantity to sell, 0 is hold, positives mean buy\n",
    "    self.action_space = spaces.Box(low=-1, high=1, shape=(1, ), dtype=np.float32)\n",
    "    # Observation space: normalized last price, normalized shares held, normalized cash balance\n",
    "    self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.current_step = 0\n",
    "    self.total_profit = 0\n",
    "    self.initial_balance = 10000\n",
    "    self.cash_balance = self.initial_balance\n",
    "    self.shares_held = 0\n",
    "    self.portfolio_value = self.cash_balance\n",
    "\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    # For plotting\n",
    "    self.portfolio_history = [self.portfolio_value]\n",
    "\n",
    "    return self.get_observation()\n",
    "\n",
    "  def get_observation(self):\n",
    "    current_price = self.df.loc[self.current_step, 'Last Price']\n",
    "    return np.array([current_price, self.shares_held, self.cash_balance])\n",
    "\n",
    "  # Action space is one number between -1 and 1.\n",
    "  def take_action(self, action_value):\n",
    "    current_price = self.df.loc[self.current_step, 'Last Price']\n",
    "    if action_value > 0:\n",
    "      self.buy_stock(action_value, current_price)\n",
    "    elif action_value < 0:\n",
    "      self.sell_stock(action_value, current_price)\n",
    "    # Update the portfolio value\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    self.portfolio_value = self.cash_balance + (self.shares_held * current_price)\n",
    "\n",
    "  def buy_stock(self, num_stocks, current_price):\n",
    "    self.cash_balance -= (num_stocks * current_price)\n",
    "    self.shares_held += num_stocks\n",
    "\n",
    "  def sell_stock(self, num_stocks, current_price):\n",
    "    self.balance += (num_stocks * current_price)\n",
    "    self.shares_held -= num_stocks\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_step += 1\n",
    "    self.take_action(action)\n",
    "    reward = self.calculate_reward(action)\n",
    "    terminated = (self.current_step >= len(self.prices) - 1)\n",
    "    observation = self.get_observation()\n",
    "    info = {'current_step': self.current_step, 'portfolio_value': self.portfolio_value}\n",
    "    self.portfolio_history.append(self.portfolio_value)\n",
    "    return observation, reward, terminated, info\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    if mode == 'human':\n",
    "      print(f\"Step: {self.current_step}, Portfolio Value: {self.portfolio_value}\")\n",
    "\n",
    "  # The reward is the change in portfolio value the next time_step\n",
    "  def calculate_reward(self):\n",
    "    reward = self.current_portfolio_value - self.previous_portfolio_value\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This is for the gradient descent, we will probably put this in a learn method in the DDPG class\u001b[39;00m\n\u001b[1;32m     61\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m\n\u001b[0;32m---> 62\u001b[0m optimizer_critic \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m optimizer_actor \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(actor_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Loop for each episode:\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    259\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# Because DDPG is an off-policy actor-critic policy-gradient algorithm, this means that the critic\n",
    "# evaluates the actor, but because they are off-policy, the agents act according to their own\n",
    "# policy but learn according to the target policy.\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialise environment:\n",
    "env = DDPGTradingEnv(df)\n",
    "\n",
    "# Pseudocode line 1:\n",
    "# Initialize replay memory ğ· to capacity ğ‘\n",
    "N = 1000\n",
    "replay_memory = []\n",
    "# replay_memory = deque(maxlen=N)\n",
    "# Initialise minibatch size\n",
    "minibatch_size = 50\n",
    "\n",
    "# Input: Current state\n",
    "# Output: Action to be taken\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "    def forward(self, state):\n",
    "        return 0.5\n",
    "    \n",
    "# Input: Current State & Actor's action\n",
    "# Output: Expected value of state-action pair\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return 0.5\n",
    "    \n",
    "# Pseudocode line 2 & 3:\n",
    "# Initialize actor network ğœ‹Ì‚ 1 with parameters ğ’˜1 arbitrarily\n",
    "# Initialize target actor network ğœ‹Ì‚ 2 with parameters ğ’˜2=ğ’˜1\n",
    "state_dim = 3\n",
    "action_dim = 1\n",
    "actor_model = Actor(state_dim, action_dim)\n",
    "target_actor_model = Actor(state_dim, action_dim)\n",
    "\n",
    "# Pseudocode line 4 & 5:\n",
    "# Initialize critic network ğ‘Ì‚ 1 with parameters ğœ½1 arbitrarily\n",
    "# Initialize target critic network ğ‘Ì‚ 2 with parameters ğœ½2=ğœ½1\n",
    "critic_model = Critic(state_dim + action_dim, 1)\n",
    "target_critic_model = Critic(state_dim + action_dim, 1)\n",
    "\n",
    "# Pseudocode line 6: Algorithm parameters: target network learning rate ğ›½>0\n",
    "beta = 0.005\n",
    "# Gamma is the discount factor (importance of future rewards)\n",
    "gamma = 0.9\n",
    "\n",
    "# This is for the gradient descent, we will probably put this in a learn method in the DDPG class\n",
    "lr = 0.005\n",
    "optimizer_critic = optim.Adam(critic_model.parameters(), lr=lr)\n",
    "optimizer_actor = optim.Adam(actor_model.parameters(), lr=lr)\n",
    "\n",
    "# Loop for each episode:\n",
    "num_episodes = 100\n",
    "for i in range(num_episodes):\n",
    "    # Initialise random process Ïµ for action exploration\n",
    "    epsilon = 0.1\n",
    "    # Initialise S\n",
    "    state = env.reset()\n",
    "\n",
    "    # Loop for each step of episode (each time step in df)\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        # Select action ğ´â†ğœ‹Ì‚ 1(ğ‘†,ğ’˜1)+îˆº\n",
    "        action = actor_model.forward(state)\n",
    "        # Execute action ğ´, observe reward ğ‘… and next-state ğ‘†â€²\n",
    "        new_state, reward, terminated, info = env.step(action)\n",
    "        # Store transition (ğ‘†,ğ´,ğ‘…,ğ‘†â€²) in ğ·\n",
    "        transition = (state, action, reward, new_state)\n",
    "        replay_memory.append(transition)\n",
    "        # For each transition (ğ‘†ğ‘—,ğ´ğ‘—,ğ‘…ğ‘—,ğ‘†â€²ğ‘—) in minibatch sampled from ğ·:\n",
    "        minibatch_sampled_from_D = random.sample(replay_memory, minibatch_size)\n",
    "        for transition in minibatch_sampled_from_D:\n",
    "            # ğ‘¦â†ğ‘…ğ‘—+ğ›¾ğ‘Ì‚ 2(ğ‘†â€²ğ‘—,ğœ‹Ì‚ 2(ğ‘†â€²ğ‘—,ğ’˜2),ğœ½2)\n",
    "            state, action, reward, new_state = transition\n",
    "            # ğœ‹Ì‚ 2(ğ‘†â€²ğ‘—,ğ’˜2)\n",
    "            target_actor_model_output = target_actor_model.forward(new_state)\n",
    "            y = reward + gamma * target_critic_model.forward(new_state, target_actor_model_output)\n",
    "            # Perform gradient descent step âˆ‡ğœ½1(ğ‘¦âˆ’ğ‘Ì‚ 1(ğ‘†ğ‘—,ğ´ğ‘—,ğœ½1))2 for critic (critic learning from target critic)\n",
    "            critic_output = critic_model.forward(state, action)\n",
    "            critic_loss = F.mse_loss(critic_output, y)\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer_critic.step()\n",
    "            # Perform gradient ascent step âˆ‡ğ‘¤1ğ¸[ğ‘Ì‚ 1(ğ‘†ğ‘—,ğœ‹Ì‚ 1(ğ‘†ğ‘—,ğ’˜1),ğœ½1)] for actor (actor learning from critic)\n",
    "            actor_output = actor_model.forward(state)\n",
    "            actor_loss = -torch.mean(critic_model(state, actor_output))\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "        # Update target actor network parameters ğ’˜2â†ğ›½ğ’˜1+(1âˆ’ğ›½)ğ’˜2\n",
    "        \n",
    "        # Update target critic network parameters ğœ½2â†ğ›½ğœ½1+(1âˆ’ğ›½)ğœ½2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Class TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Current state\n",
    "# Output: Action to be taken\n",
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "    super().__init__()\n",
    "\n",
    "    input_dim = input_dim[0]\n",
    "    output_dim = output_dim[0]\n",
    "\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc0(state))\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.tanh(self.fc2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Class TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Current State & Actor's action\n",
    "# Output: Expected value of state-action pair\n",
    "class Critic(nn.Module):\n",
    "\n",
    "  def init(self, input_dim, output_dim, hidden_size=128):\n",
    "\n",
    "    input_dim = input_dim[0]  # Assuming output_dim represents the action dimension\n",
    "    action_dim = 2\n",
    "\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim + action_dim, hidden_size)  # Combine state and action dimensions\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    x_state = self.relu(self.fc0(state))\n",
    "    x = torch.cat((x_state, action), dim=1)  # Concatenate state and action\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise environment\n",
    "env = DDPGTradingEnv(df)\n",
    "\n",
    "# State space shape\n",
    "state_dim = env.observation_space.shape\n",
    "# Action space shape (should be 1)\n",
    "action_dim = env.action_space.shape\n",
    "\n",
    "# Initialise models\n",
    "actor_model = Actor(input_dim=state_dim, output_dim=action_dim)\n",
    "critic_model = Critic(input_dim=state_dim, output_dim=1)\n",
    "\n",
    "# Example forward pass\n",
    "state = torch.FloatTensor(\n",
    "    env._get_observation())  # Assuming _get_observation() returns the state\n",
    "action = actor_model.forward(state)\n",
    "value_estimate = critic_model.forward(state, action)\n",
    "\n",
    "print(value_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the profit change\n",
    "plt.plot(env.portfolio_history)\n",
    "plt.title(\"Profit Change Over Time\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Total Profit\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Convolution Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
