{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Date  Last Price    Volume  SMAVG (15)\n",
      "2331 2023-03-07 20:00:00    0.596608  0.049707    0.072420\n",
      "2330 2023-03-07 20:30:00    0.606681  0.165330    0.086878\n",
      "2329 2023-03-08 14:30:00    0.634154  0.134774    0.099523\n",
      "2328 2023-03-08 15:00:00    0.608055  0.086473    0.077523\n",
      "2327 2023-03-08 15:30:00    0.571654  0.099766    0.067301\n",
      "...                  ...         ...       ...         ...\n",
      "473  2023-09-29 20:30:00    0.881409  0.306496    0.180971\n",
      "472  2023-10-02 14:30:00    0.794870  0.171930    0.194260\n",
      "471  2023-10-02 15:00:00    0.755034  0.144825    0.184963\n",
      "470  2023-10-02 15:30:00    0.762818  0.113712    0.166506\n",
      "469  2023-10-02 16:00:00    0.764192  0.092697    0.157100\n",
      "\n",
      "[1863 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "#C:\\Users\\rohit\\OneDrive\\Documents\\Asset-Pricing-with-Reinforcement-Learning\\XOM_30_minute_6_month_data.csv\n",
    "df = pd.read_csv('XOM_30_minute_6_month_data.csv', parse_dates=['Date'])\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "df[['Last Price', 'Volume', 'SMAVG (15)']] = scaler.fit_transform(df[['Last Price', 'Volume', 'SMAVG (15)']])\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(train_df[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class DDPGTradingEnv(gymnasium.Env):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.df = df\n",
    "    self.current_step = 0\n",
    "    self.total_steps = len(df) - 1\n",
    "\n",
    "    # Negatives mean quantity to sell, 0 is hold, positives mean buy\n",
    "    self.action_space = spaces.Box(low=-1, high=1, shape=(1, ), dtype=np.float32)\n",
    "    # Observation space: normalized last price, normalized shares held, normalized cash balance\n",
    "    self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.current_step = 0\n",
    "    self.total_profit = 0\n",
    "    self.initial_balance = 10000\n",
    "    self.cash_balance = self.initial_balance\n",
    "    self.shares_held = 0\n",
    "    self.portfolio_value = self.cash_balance\n",
    "\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    # For plotting\n",
    "    self.portfolio_history = [self.portfolio_value]\n",
    "\n",
    "    return self.get_observation()\n",
    "\n",
    "  def get_observation(self):\n",
    "    current_price = self.df.loc[self.current_step, 'Last Price']\n",
    "    return np.array([current_price, self.shares_held, self.cash_balance])\n",
    "\n",
    "  # Action space is one number between -1 and 1.\n",
    "  def take_action(self, action_value):\n",
    "    current_price = self.df.loc[self.current_step, 'Last Price']\n",
    "    if action_value > 0:\n",
    "      self.buy_stock(action_value, current_price)\n",
    "    elif action_value < 0:\n",
    "      self.sell_stock(action_value, current_price)\n",
    "    # Update the portfolio value\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    self.portfolio_value = self.cash_balance + (self.shares_held * current_price)\n",
    "\n",
    "  def buy_stock(self, num_stocks, current_price):\n",
    "    self.cash_balance -= (num_stocks * current_price)\n",
    "    self.shares_held += num_stocks\n",
    "\n",
    "  def sell_stock(self, num_stocks, current_price):\n",
    "    self.balance += (num_stocks * current_price)\n",
    "    self.shares_held -= num_stocks\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_step += 1\n",
    "    self.take_action(action)\n",
    "    reward = self.calculate_reward(action)\n",
    "    terminated = (self.current_step >= len(self.prices) - 1)\n",
    "    observation = self.get_observation()\n",
    "    info = {'current_step': self.current_step, 'portfolio_value': self.portfolio_value}\n",
    "    self.portfolio_history.append(self.portfolio_value)\n",
    "    return observation, reward, terminated, info\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    if mode == 'human':\n",
    "      print(f\"Step: {self.current_step}, Portfolio Value: {self.portfolio_value}\")\n",
    "\n",
    "  # The reward is the change in portfolio value the next time_step\n",
    "  def calculate_reward(self):\n",
    "    reward = self.current_portfolio_value - self.previous_portfolio_value\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DDPGTradingEnv.calculate_reward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m action \u001b[38;5;241m=\u001b[39m actor_model\u001b[38;5;241m.\u001b[39mforward(state)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Execute action ğ´, observe reward ğ‘… and next-state ğ‘†â€²\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m new_state, reward, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Store transition (ğ‘†,ğ´,ğ‘…,ğ‘†â€²) in ğ·\u001b[39;00m\n\u001b[1;32m     93\u001b[0m transition \u001b[38;5;241m=\u001b[39m (state, action, reward, new_state)\n",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m, in \u001b[0;36mDDPGTradingEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake_action(action)\n\u001b[0;32m---> 67\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m terminated \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprices) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()\n",
      "\u001b[0;31mTypeError\u001b[0m: DDPGTradingEnv.calculate_reward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Because DDPG is an off-policy actor-critic policy-gradient algorithm, this means that the critic\n",
    "# evaluates the actor, but because they are off-policy, the agents act according to their own\n",
    "# policy but learn according to the target policy.\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialise environment:\n",
    "env = DDPGTradingEnv(df)\n",
    "\n",
    "# Pseudocode line 1:\n",
    "# Initialize replay memory ğ· to capacity ğ‘\n",
    "N = 1000\n",
    "replay_memory = []\n",
    "# replay_memory = deque(maxlen=N)\n",
    "# Initialise minibatch size\n",
    "minibatch_size = 50\n",
    "\n",
    "# Input: Current state\n",
    "# Output: Action to be taken\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return 0.5\n",
    "    \n",
    "# Input: Current State & Actor's action\n",
    "# Output: Expected value of state-action pair\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1_state = nn.Linear(state_dim, 64)\n",
    "        self.fc2_state = nn.Linear(64, 64)\n",
    "        self.fc2_action = nn.Linear(action_dim, 64)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return 0.5\n",
    "    \n",
    "# Pseudocode line 2 & 3:\n",
    "# Initialize actor network ğœ‹Ì‚ 1 with parameters ğ’˜1 arbitrarily\n",
    "# Initialize target actor network ğœ‹Ì‚ 2 with parameters ğ’˜2=ğ’˜1\n",
    "state_dim = 3\n",
    "action_dim = 1\n",
    "actor_model = Actor(state_dim, action_dim)\n",
    "target_actor_model = Actor(state_dim, action_dim)\n",
    "\n",
    "# Pseudocode line 4 & 5:\n",
    "# Initialize critic network ğ‘Ì‚ 1 with parameters ğœ½1 arbitrarily\n",
    "# Initialize target critic network ğ‘Ì‚ 2 with parameters ğœ½2=ğœ½1\n",
    "critic_model = Critic(state_dim + action_dim, 1)\n",
    "target_critic_model = Critic(state_dim + action_dim, 1)\n",
    "\n",
    "# Pseudocode line 6: Algorithm parameters: target network learning rate ğ›½>0\n",
    "beta = 0.005\n",
    "# Gamma is the discount factor (importance of future rewards)\n",
    "gamma = 0.9\n",
    "\n",
    "# This is for the gradient descent, we will probably put this in a learn method in the DDPG class\n",
    "lr = 0.005\n",
    "optimizer_critic = optim.Adam(critic_model.parameters(), lr=lr)\n",
    "optimizer_actor = optim.Adam(actor_model.parameters(), lr=lr)\n",
    "\n",
    "# Loop for each episode:\n",
    "num_episodes = 100\n",
    "for i in range(num_episodes):\n",
    "    # Initialise random process Ïµ for action exploration\n",
    "    epsilon = 0.1\n",
    "    # Initialise S\n",
    "    state = env.reset()\n",
    "\n",
    "    # Loop for each step of episode (each time step in df)\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        # Select action ğ´â†ğœ‹Ì‚ 1(ğ‘†,ğ’˜1)+îˆº\n",
    "        action = actor_model.forward(state)\n",
    "        # Execute action ğ´, observe reward ğ‘… and next-state ğ‘†â€²\n",
    "        new_state, reward, terminated, info = env.step(action)\n",
    "        # Store transition (ğ‘†,ğ´,ğ‘…,ğ‘†â€²) in ğ·\n",
    "        transition = (state, action, reward, new_state)\n",
    "        replay_memory.append(transition)\n",
    "        # For each transition (ğ‘†ğ‘—,ğ´ğ‘—,ğ‘…ğ‘—,ğ‘†â€²ğ‘—) in minibatch sampled from ğ·:\n",
    "        minibatch_sampled_from_D = random.sample(replay_memory, minibatch_size)\n",
    "        for transition in minibatch_sampled_from_D:\n",
    "            # ğ‘¦â†ğ‘…ğ‘—+ğ›¾ğ‘Ì‚ 2(ğ‘†â€²ğ‘—,ğœ‹Ì‚ 2(ğ‘†â€²ğ‘—,ğ’˜2),ğœ½2)\n",
    "            state, action, reward, new_state = transition\n",
    "            # ğœ‹Ì‚ 2(ğ‘†â€²ğ‘—,ğ’˜2)\n",
    "            target_actor_model_output = target_actor_model.forward(new_state)\n",
    "            y = reward + gamma * target_critic_model.forward(new_state, target_actor_model_output)\n",
    "            # Perform gradient descent step âˆ‡ğœ½1(ğ‘¦âˆ’ğ‘Ì‚ 1(ğ‘†ğ‘—,ğ´ğ‘—,ğœ½1))2 for critic (critic learning from target critic)\n",
    "            critic_output = critic_model.forward(state, action)\n",
    "            critic_loss = F.mse_loss(critic_output, y)\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer_critic.step()\n",
    "            # Perform gradient ascent step âˆ‡ğ‘¤1ğ¸[ğ‘Ì‚ 1(ğ‘†ğ‘—,ğœ‹Ì‚ 1(ğ‘†ğ‘—,ğ’˜1),ğœ½1)] for actor (actor learning from critic)\n",
    "            actor_output = actor_model.forward(state)\n",
    "            actor_loss = -torch.mean(critic_model(state, actor_output))\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "        # Update target actor network parameters ğ’˜2â†ğ›½ğ’˜1+(1âˆ’ğ›½)ğ’˜2\n",
    "        for target_param, param in zip(target_actor_model.parameters(), actor_model.parameters()):\n",
    "            target_param.data.copy_(beta * param.data + (1 - beta) * target_param.data)    \n",
    "        # Update target critic network parameters ğœ½2â†ğ›½ğœ½1+(1âˆ’ğ›½)ğœ½2\n",
    "        for target_param, param in zip(target_critic_model.parameters(), critic_model.parameters()):\n",
    "            target_param.data.copy_(beta * param.data + (1 - beta) * target_param.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Class TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Current state\n",
    "# Output: Action to be taken\n",
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "    super().__init__()\n",
    "\n",
    "    input_dim = input_dim[0]\n",
    "    output_dim = output_dim[0]\n",
    "\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc0(state))\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.tanh(self.fc2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Class TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Current State & Actor's action\n",
    "# Output: Expected value of state-action pair\n",
    "class Critic(nn.Module):\n",
    "\n",
    "  def init(self, input_dim, output_dim, hidden_size=128):\n",
    "\n",
    "    input_dim = input_dim[0]  # Assuming output_dim represents the action dimension\n",
    "    action_dim = 2\n",
    "\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim + action_dim, hidden_size)  # Combine state and action dimensions\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    x_state = self.relu(self.fc0(state))\n",
    "    x = torch.cat((x_state, action), dim=1)  # Concatenate state and action\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise environment\n",
    "env = DDPGTradingEnv(df)\n",
    "\n",
    "# State space shape\n",
    "state_dim = env.observation_space.shape\n",
    "# Action space shape (should be 1)\n",
    "action_dim = env.action_space.shape\n",
    "\n",
    "# Initialise models\n",
    "actor_model = Actor(input_dim=state_dim, output_dim=action_dim)\n",
    "critic_model = Critic(input_dim=state_dim, output_dim=1)\n",
    "\n",
    "# Example forward pass\n",
    "state = torch.FloatTensor(\n",
    "    env._get_observation())  # Assuming _get_observation() returns the state\n",
    "action = actor_model.forward(state)\n",
    "value_estimate = critic_model.forward(state, action)\n",
    "\n",
    "print(value_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the profit change\n",
    "plt.plot(env.portfolio_history)\n",
    "plt.title(\"Profit Change Over Time\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Total Profit\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Convolution Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
