{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Date  Last Price    Volume  SMAVG (15)\n",
      "2331 2023-03-07 20:00:00    0.596608  0.049707    0.072420\n",
      "2330 2023-03-07 20:30:00    0.606681  0.165330    0.086878\n",
      "2329 2023-03-08 14:30:00    0.634154  0.134774    0.099523\n",
      "2328 2023-03-08 15:00:00    0.608055  0.086473    0.077523\n",
      "2327 2023-03-08 15:30:00    0.571654  0.099766    0.067301\n",
      "...                  ...         ...       ...         ...\n",
      "473  2023-09-29 20:30:00    0.881409  0.306496    0.180971\n",
      "472  2023-10-02 14:30:00    0.794870  0.171930    0.194260\n",
      "471  2023-10-02 15:00:00    0.755034  0.144825    0.184963\n",
      "470  2023-10-02 15:30:00    0.762818  0.113712    0.166506\n",
      "469  2023-10-02 16:00:00    0.764192  0.092697    0.157100\n",
      "\n",
      "[1863 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "#C:\\Users\\rohit\\OneDrive\\Documents\\Asset-Pricing-with-Reinforcement-Learning\\XOM_30_minute_6_month_data.csv\n",
    "df = pd.read_csv('XOM_30_minute_6_month_data.csv', parse_dates=['Date'])\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "df[['Last Price', 'Volume', 'SMAVG (15)']] = scaler.fit_transform(df[['Last Price', 'Volume', 'SMAVG (15)']])\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(train_df[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 1), grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class DDPGTradingEnv(gymnasium.Env):\n",
    "\n",
    "  def __init__(self, df):\n",
    "\n",
    "    self.df = df\n",
    "\n",
    "    # Extract prices and features\n",
    "    self.prices = df['Last Price'].values\n",
    "    # features = df[['Last Price', 'Volume', 'SMAVG (15)']].values\n",
    "    self.features = df[['Last Price']].values\n",
    "\n",
    "    # Negatives mean quantity to sell, 0 is hold, positives mean buy\n",
    "    self.action_space = spaces.Box(low=-1, high=1, shape=(1, ), dtype=np.float32)\n",
    "    # self.observation_space = spaces.Box(low=0, high=1, shape=self.shape, dtype=np.float32)\n",
    "\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self, seed=None):\n",
    "    super().reset(seed=1)\n",
    "    self.current_step = 0\n",
    "    self.total_profit = 0\n",
    "    self.initial_balance = 10000\n",
    "    self.cash_balance = self.initial_balance\n",
    "    self.shares_held = 0\n",
    "    self.portfolio_value = self.cash_balance\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    # For plotting\n",
    "    self.portfolio_history = [self.portfolio_value]\n",
    "\n",
    "  def get_observation(self):\n",
    "    current_price = self.prices[self.current_step]\n",
    "    return [current_price, self.shares_held, self.cash_balance]\n",
    "\n",
    "  def take_action(self, action_value):\n",
    "    current_price = self.prices[self.current_step]\n",
    "    if action_value > 0:\n",
    "      self.buy_stock(action_value, current_price)\n",
    "    elif action_value < 0:\n",
    "      self.sell_stock(action_value, current_price)\n",
    "    # Update the portfolio value\n",
    "    self.previous_portfolio_value = self.portfolio_value\n",
    "    self.portfolio_value = self.cash_balance + (self.shares_held * current_price)\n",
    "\n",
    "  def buy_stock(self, num_stocks, current_price):\n",
    "    self.cash_balance -= (num_stocks * current_price)\n",
    "    self.shares_held += num_stocks\n",
    "\n",
    "  def sell_stock(self, num_stocks, current_price):\n",
    "    self.balance += (num_stocks * current_price)\n",
    "    self.shares_held -= num_stocks\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_step += 1\n",
    "    self.take_action(action)\n",
    "    reward = self.calculate_reward(action)\n",
    "    terminated = (self.current_step >= len(self.prices) - 1)\n",
    "    observation = self.get_observation()\n",
    "    info = {\n",
    "        'current_step': self.current_step,\n",
    "        'portfolio_value': self.portfolio_value\n",
    "    }\n",
    "    self.portfolio_history.append(self.portfolio_value)\n",
    "    return observation, reward, terminated, info\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    if mode == 'human':\n",
    "      print(f\"Step: {self.current_step}, Portfolio Value: {self.portfolio_value}\")\n",
    "\n",
    "  # The reward is the change in portfolio value the next time_step\n",
    "  def calculate_reward(self):\n",
    "    reward = self.current_portfolio_value - self.previous_portfolio_value\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise environment\n",
    "env = DDPGTradingEnv(df)\n",
    "\n",
    "# Example usage in your DDPGTradingEnv\n",
    "state_dim = env.observation_space.shape  # Adjust based on your environment's state shape\n",
    "action_dim = env.action_space.shape  # Adjust based on your environment's action shape\n",
    "\n",
    "actor_model = Actor(input_dim=state_dim, output_dim=action_dim)\n",
    "critic_model = Critic(input_dim=state_dim, output_dim=1)\n",
    "\n",
    "# Example forward pass\n",
    "state = torch.FloatTensor(\n",
    "    env._get_observation())  # Assuming _get_observation() returns the state\n",
    "action = actor_model.forward(state)\n",
    "value_estimate = critic_model.forward(state)\n",
    "\n",
    "print(value_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "    super(Actor, self).__init__()\n",
    "\n",
    "    input_dim = input_dim[0]\n",
    "    output_dim = output_dim[0]\n",
    "\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc0(state))\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.tanh(self.fc2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim, hidden_size=128):\n",
    "    super(Critic, self).__init__()\n",
    "\n",
    "    input_dim = input_dim[0]\n",
    "    self.fc0 = nn.Linear(1, input_dim)\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc0(state))\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the profit change\n",
    "plt.plot(env.portfolio_history)\n",
    "plt.title(\"Profit Change Over Time\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Total Profit\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Convolution Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
